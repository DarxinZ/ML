{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T10_1_2.ipynb",
      "provenance": [],
      "mount_file_id": "18SWDm-49P0wVh4OsTMZjiG80Ha7KI2O1",
      "authorship_tag": "ABX9TyMRdlJdupDZq1/CIe6cJ9Gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarxinZ/ML/blob/main/T10_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFrE2QXunw6U"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "class ITrainable:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_propagation(self, prev_A):\n",
        "        raise NotImplementedError(\"forward_propagation not implemented: ITrainable is an interface\")\n",
        "\n",
        "    def backward_propagation(self, dA):\n",
        "        raise NotImplementedError(\"backward_propagation not implemented: ITrainable is an interface\")\n",
        "\n",
        "    def update_parameters(self):\n",
        "        raise NotImplementedError(\"update_parameters not implemented: ITrainable is an interface\")\n",
        "\n",
        "    def save_parameters(self, file_path):\n",
        "        pass\n",
        "\n",
        "    def restore_parameters(self, file_path):\n",
        "        pass\n",
        "\n",
        "class DLLinearFamily(ITrainable):\n",
        "    def __init__(self,name, n, alpha, optimization=None):\n",
        "        self.name = name\n",
        "        self.alpha = alpha\n",
        "        self.optimization = optimization\n",
        "        self.W = np.zeros((n,1),dtype=float)\n",
        "        self.b = 0\n",
        "        # optimization parameters\n",
        "        if self.optimization == 'adaptive':\n",
        "            self.adaptive_cont = 1.1\n",
        "            self.adaptive_switch = 0.5\n",
        "            self.adaptive_W = np.full(self.W.shape,alpha,dtype=float)\n",
        "            self.adaptive_b = alpha\n",
        "            \n",
        "    def __str__(self):\n",
        "        s = f\"{self.name} Function:\\n\"\n",
        "        s += f\"\\tlearning_rate (alpha): {self.alpha}\\n\"\n",
        "        if self.optimization != None:\n",
        "            s += f\"\\tOptimization: {self.optimization}\\n\"\n",
        "            if self.optimization == \"adaptive\":\n",
        "                s += f\"\\t\\tadaptive parameters:\\n\"\n",
        "                s += f\"\\t\\t\\tcont: {self.adaptive_cont}\\n\"\n",
        "                s += f\"\\t\\t\\tswitch: {self.adaptive_switch}\\n\"\n",
        "        s += \"\\tParameters:\\n\"\n",
        "        s += f\"\\t\\tW shape: {self.W.shape}\\n\"\n",
        "        s += f\"\\t\\tb: {self.b}\\n\"\n",
        "        return s;\n",
        "\n",
        "\n",
        "    def forward_propagation(self, prev_A):\n",
        "        self.prev_A = np.copy(prev_A)\n",
        "        Z = self.W.T@prev_A+self.b\n",
        "        return Z\n",
        "\n",
        "    def backward_propagation(self, dZ):\n",
        "        db_m_values = dZ * np.full((1,self.prev_A.shape[1]),1)\n",
        "        dW_n_m_values = dZ * self.prev_A\n",
        "        self.db = np.sum(db_m_values, keepdims=True, axis=1)\n",
        "        self.dW = np.sum(dW_n_m_values, keepdims=True, axis=1)\n",
        "    \n",
        "    def update_parameters(self):\n",
        "        if self.optimization == 'adaptive':\n",
        "            self.adaptive_W *= np.where(self.adaptive_W * self.dW > 0, self.adaptive_cont, -self.adaptive_switch)\n",
        "            self.W -= self.adaptive_W \n",
        "            if self.adaptive_b * self.db > 0:\n",
        "                self.adaptive_b *= self.adaptive_cont\n",
        "            else:\n",
        "                self.adaptive_b *= -self.adaptive_switch\n",
        "            self.b -= self.adaptive_b \n",
        "        else:\n",
        "            self.W -= self.alpha * self.dW\n",
        "            self.b -= self.alpha * self.db\n",
        "\n",
        "class DLLinearLayer(ITrainable):\n",
        "    def __init__(self,name, num_units,input_size, alpha,optimization=None,initialization=None):\n",
        "        self.name = name\n",
        "        self.alpha = alpha\n",
        "        self.num_units = num_units\n",
        "        self.input_size = input_size\n",
        "        self.optimization = optimization\n",
        "        #self.W = np.zeros((n,1),dtype=float)\n",
        "        self.W = DLLinearLayer.normal_initialization((num_units,input_size), factor=1)\n",
        "\n",
        "        if initialization == \"He\":\n",
        "            self.W = DLLinearLayer.normal_initialization((num_units,input_size), factor=1)\n",
        "            self.W = DLLinearLayer.W_he_initialization(self)\n",
        "        elif initialization == \"Xavier\":\n",
        "            self.W = DLLinearLayer.normal_initialization((num_units,input_size), factor=1)\n",
        "            self.W = DLLinearLayer.W_Xaviar_initialization(self)\n",
        "        else:\n",
        "            self.W = DLLinearLayer.normal_initialization((num_units,input_size), factor=0.01)\n",
        "\n",
        "        self.b = np.zeros((num_units,1),dtype=float)\n",
        "        # optimization parameters\n",
        "        if self.optimization == 'adaptive':\n",
        "            self.adaptive_cont = 1.1\n",
        "            self.adaptive_switch = 0.5\n",
        "            self.adaptive_W = np.full(self.W.shape,alpha,dtype=float)\n",
        "            self.adaptive_b = np.full(self.b.shape,alpha,dtype=float)\n",
        "            #self.adaptive_b = alpha\n",
        "    \n",
        "    def __str__(self):\n",
        "        s = f\"{self.name} Function:\\n\"\n",
        "        s += f\"\\tlearning_rate (alpha): {self.alpha}\\n\"\n",
        "        if self.optimization != None:\n",
        "            s += f\"\\tOptimization: {self.optimization}\\n\"\n",
        "            if self.optimization == \"adaptive\":\n",
        "                s += f\"\\t\\tadaptive parameters:\\n\"\n",
        "                s += f\"\\t\\t\\tcont: {self.adaptive_cont}\\n\"\n",
        "                s += f\"\\t\\t\\tswitch: {self.adaptive_switch}\\n\"\n",
        "        s += \"\\tParameters:\\n\"\n",
        "        s += f\"\\t\\tW shape: {self.W.shape}\\n\"\n",
        "        s += f\"\\t\\tb: {self.b.shape}\\n\"\n",
        "        s += f\"\\t\\tnum units: {self.num_units}\\n\"\n",
        "        s += f\"\\t\\tinput size: {self.input_size}\\n\"\n",
        "        return s;\n",
        "\n",
        "\n",
        "    def forward_propagation(self, prev_A):\n",
        "        self.prev_A = np.copy(prev_A)\n",
        "        #Z = self.W.T@prev_A+self.b\n",
        "        Z = self.W@prev_A+self.b\n",
        "        return Z\n",
        "\n",
        "    def backward_propagation(self, dZ):\n",
        "        db_m_values = dZ * np.full((1,self.prev_A.shape[1]),1)\n",
        "        #dW_n_m_values = dZ * self.prev_A\n",
        "        #self.db = np.sum(db_m_values, keepdims=True, axis=1)\n",
        "        #self.dW = np.sum(dW_n_m_values, keepdims=True, axis=1)\n",
        "        self.dW = dZ@self.prev_A.T\n",
        "        self.db = np.sum(db_m_values, keepdims=True, axis=1)\n",
        "        dA_prev = self.W.T@dZ\n",
        "        return dA_prev\n",
        "\n",
        "\n",
        "    \n",
        "    def update_parameters(self):\n",
        "        if self.optimization == 'adaptive':\n",
        "            self.adaptive_W *= np.where(self.adaptive_W * self.dW > 0, self.adaptive_cont, -self.adaptive_switch)\n",
        "            self.W -= self.adaptive_W \n",
        "            #if self.adaptive_b * self.db > 0:\n",
        "            #    self.adaptive_b *= self.adaptive_cont\n",
        "            #else:\n",
        "            #    self.adaptive_b *= -self.adaptive_switch\n",
        "            #self.b -= self.adaptive_b \n",
        "            self.adaptive_b *= np.where(self.adaptive_b * self.db > 0, self.adaptive_cont, -self.adaptive_switch)\n",
        "            self.b -= self.adaptive_b \n",
        "        else:\n",
        "            self.W -= self.alpha * self.dW\n",
        "            self.b -= self.alpha * self.db\n",
        "\n",
        "    def W_he_initialization(self):\n",
        "        W1 = self.W * np.sqrt(2/self.input_size)\n",
        "        return W1\n",
        "\n",
        "    def W_Xaviar_initialization(self):\n",
        "        W1 = self.W * np.sqrt(1/self.input_size)\n",
        "        return W1\n",
        "\n",
        "    def save_parameters(self, file_path):\n",
        "        file_name = file_path+\"/\"+self.name+\".h5\"\n",
        "        with h5py.File(file_name, 'w') as hf:\n",
        "            hf.create_dataset(\"W\",  data=self.W)\n",
        "            hf.create_dataset(\"b\",  data=self.b)\n",
        "\n",
        "    def restore_parameters(self, file_path):\n",
        "        file_name = file_path+\"/\"+self.name+\".h5\"\n",
        "        with h5py.File(file_name, 'r') as hf:\n",
        "            self.W = hf['W'][:]\n",
        "            self.b = hf['b'][:]\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_initialization(shape,factor=0.01): \n",
        "        ##W1 = np.random.randn(shape[0],shape[1]) * factor\n",
        "        W1 = np.random.randn(*shape) * factor\n",
        "        return W1\n",
        "\n",
        "\n",
        "\n",
        "class DLNetwork(ITrainable):\n",
        "    def __init__(self,name):\n",
        "        self.name = name\n",
        "        self.layers = []\n",
        "\n",
        "    def __str__(self):\n",
        "        s = f\"{self.name}:\\n\"\n",
        "        for l in self.layers:\n",
        "            s += str(l)\n",
        "        return s\n",
        "\n",
        "    def add(self,iTrainable):\n",
        "        for l in self.layers:\n",
        "            if l.name == iTrainable.name:\n",
        "                raise ValueError(f\"{iTrainable.name} already exists\")\n",
        "        self.layers.append(iTrainable)\n",
        "\n",
        "    def forward_propagation(self,X):\n",
        "        Al=X\n",
        "        for l in self.layers:\n",
        "           Al = l.forward_propagation(Al)\n",
        "        return Al\n",
        "\n",
        "    def backward_propagation(self, dY_hat):\n",
        "        dAl = dY_hat\n",
        "        for l in reversed(self.layers):\n",
        "           dAl = l.backward_propagation(dAl)\n",
        "        return dAl\n",
        "\n",
        "    def update_parameters(self):\n",
        "        for l in self.layers:\n",
        "           l.update_parameters()\n",
        "\n",
        "    def save_parameters(self, directory_path):\n",
        "        directory = directory_path+\"/\"+self.name\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        for l in self.layers:\n",
        "           l.save_parameters(directory)\n",
        "\n",
        "    def restore_parameters(self, directory_path):\n",
        "        directory = directory_path+\"/\"+self.name\n",
        "        #os.makedirs(directory, exist_ok=True)\n",
        "        for l in self.layers:\n",
        "           l.restore_parameters(directory)\n",
        "\n",
        "class DLActivation(ITrainable):\n",
        "#    def __init__(self):\n",
        "#        self.name = \"Sigmoid\"\n",
        "    def __init__(self, activation):\n",
        "        self.name = activation\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Activation function: \"+self.name+\"\\n\"\n",
        "    \n",
        "    def forward_propagation(self,Z):\n",
        "        if self.name == \"sigmoid\":\n",
        "            self.S = 1/(1+np.exp(-Z))\n",
        "            return self.S\n",
        "        if self.name == \"tanh\":\n",
        "            self.S = np.tanh(Z)\n",
        "            return self.S\n",
        "        if self.name == \"relu\":\n",
        "            self.Z = Z\n",
        "            return np.maximum(0,Z)\n",
        "        if self.name == \"leaky_relu\":\n",
        "            self.leakyRelu_d = 0.01\n",
        "            self.Z = Z\n",
        "            return np.where(Z <= 0, self.leakyRelu_d*Z, Z)\n",
        "        if self.name == \"softmax\":\n",
        "            A = np.exp(Z)/np.sum(np.exp(Z),keepdims=True, axis=0)\n",
        "            return A\n",
        "        raise NotImplementedError(\"Unimplemented activation:\", self.name)\n",
        "\n",
        "    def backward_propagation(self, dS):\n",
        "        if self.name == \"sigmoid\":\n",
        "            return dS * self.S * (1-self.S)\n",
        "        if self.name == \"tanh\":\n",
        "            return dS * (1-self.S * self.S)\n",
        "        if self.name == \"relu\":\n",
        "            return dS * np.where(self.Z <= 0, 0, 1)\n",
        "        if self.name == \"leaky_relu\":\n",
        "            return dS * np.where(self.Z <= 0, self.leakyRelu_d, 1)\n",
        "        if self.name == \"softmax\":\n",
        "            return dS\n",
        "        raise NotImplementedError(\"Unimplemented activation:\", self.name)\n",
        "    \n",
        "    def update_parameters(self):\n",
        "        pass\n",
        "\n",
        "class DLPerceptron(DLNetwork):\n",
        "    def __init__(self, name, n, alpha, optimization=None):\n",
        "        DLNetwork.__init__(self, name)\n",
        "        self.linear = DLLinearFamily(\"linear\", n, alpha, optimization)\n",
        "        self.sigmoid = DLActivation()\n",
        "        self.add(self.linear)\n",
        "        self.add(self.sigmoid)\n",
        "\n",
        "class DLNeuronsLayer(DLNetwork):\n",
        "    def __init__(self,name,num_units,input_size, activation, alpha,optimization=None,initialization=None):\n",
        "        DLNetwork.__init__(self, name)\n",
        "        self.linear = DLLinearLayer(\"linear\", num_units,input_size, alpha,optimization,initialization)\n",
        "        self.sigmoid = DLActivation(activation)\n",
        "        self.add(self.linear)\n",
        "        self.add(self.sigmoid)\n",
        "\n",
        "class DLModel:\n",
        "    def __init__(self,name,iTrainable, loss):\n",
        "        self.name = name\n",
        "        self.iTrainable = iTrainable\n",
        "        self.loss = loss\n",
        "        if loss == \"square_dist\":\n",
        "            self.loss_forward = self.square_dist\n",
        "            self.loss_backward = self.dSquare_dist\n",
        "        elif loss == \"cross_entropy\":\n",
        "            self.loss_forward = self.cross_entropy\n",
        "            self.loss_backward = self.dCross_entropy        \n",
        "        elif loss == \"categorical_cross_entropy\":\n",
        "            self.loss_forward = self.categorical_cross_entropy\n",
        "            self.loss_backward = self.dCategorical_cross_entropy\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unimplemented loss function: \" + loss)\n",
        "\n",
        "    def __str__(self):\n",
        "        s = self.name + \"\\n\"\n",
        "        s += \"\\tLoss function: \" + self.loss + \"\\n\"\n",
        "        s += \"\\t\"+str(self.iTrainable) + \"\\n\"\n",
        "        return s\n",
        "\n",
        "\n",
        "    def square_dist(self, Y_hat, Y):\n",
        "        errors = (Y_hat - Y)**2\n",
        "        return errors\n",
        "\n",
        "    def dSquare_dist(self, Y_hat, Y):\n",
        "        m = Y.shape[1]\n",
        "        dY_hat = 2*(Y_hat - Y)/m\n",
        "        return dY_hat\n",
        "\n",
        "    def compute_cost(self, Y_hat, Y):\n",
        "        m = Y.shape[1]\n",
        "        errors = self.loss_forward(Y_hat, Y)\n",
        "        J = np.sum(errors)\n",
        "        return J/m\n",
        "\n",
        "    def cross_entropy(self, Y_hat, Y):\n",
        "        eps = 1e-10\n",
        "        Y_hat = np.where(Y_hat==0,eps,Y_hat)\n",
        "        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)\n",
        "        logprobs = -((1 - Y)*np.log(1 - Y_hat)+Y*np.log(Y_hat))\n",
        "        return logprobs\n",
        "\n",
        "    def dCross_entropy(self, Y_hat, Y):\n",
        "        eps = 1e-10\n",
        "        Y_hat = np.where(Y_hat==0,eps,Y_hat)\n",
        "        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)\n",
        "        m = Y_hat.shape[1]\n",
        "        dY_hat =(1-Y)/(1-Y_hat)-Y/Y_hat\n",
        "        return dY_hat/m\n",
        "\n",
        "    def train(self, X, Y, num_iterations):\n",
        "        print_ind = max(num_iterations // 100, 1)\n",
        "        costs = []\n",
        "        for i in range(num_iterations):\n",
        "            Y_hat = self.forward_propagation(X)\n",
        "            self.backward_propagation(Y_hat, Y)\n",
        "            self.update_parameters()\n",
        "            #record progress\n",
        "            if i > 0 and i % print_ind == 0:\n",
        "                J = self.compute_cost(Y_hat, Y)\n",
        "                costs.append(J)\n",
        "                print(\"cost after \", str(i+1), \"updates (\"+str(i//print_ind)+\"%):\",str(J))\n",
        "        costs.append(self.compute_cost(Y_hat, Y))\n",
        "        return costs\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        return self.iTrainable.forward_propagation(X)\n",
        "\n",
        "    def backward_propagation(self, Y_hat,Y):\n",
        "        dY_hat = self.loss_backward(Y_hat, Y)\n",
        "        self.iTrainable.backward_propagation(dY_hat)\n",
        "\n",
        "    def update_parameters(self):\n",
        "        self.iTrainable.update_parameters()\n",
        "\n",
        "    def categorical_cross_entropy(self, Y_hat, Y):\n",
        "        eps = 1e-10\n",
        "        Y_hat = np.where(Y_hat==0,eps,Y_hat)\n",
        "        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)\n",
        "        logprobs = -(np.sum(Y * np.log(Y_hat), keepdims=True, axis=0))\n",
        "        return logprobs\n",
        "\n",
        "    def dCategorical_cross_entropy(self, Y_hat, Y):\n",
        "        eps = 1e-10\n",
        "        Y_hat = np.where(Y_hat==0,eps,Y_hat)\n",
        "        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)\n",
        "        m = Y_hat.shape[1]\n",
        "        dY_hat = (Y_hat - Y)/m\n",
        "        return dY_hat\n",
        "\n",
        "    def confusion_matrix(self, X, Y):\n",
        "        prediction = self.forward_propagation(X)\n",
        "        prediction_index = np.argmax(prediction, axis=0)\n",
        "        Y_index = np.argmax(Y, axis=0)\n",
        "        right = np.sum(prediction_index == Y_index)\n",
        "        print(\"accuracy: \",str(right/len(Y[0])))\n",
        "        print(confusion_matrix(prediction_index, Y_index))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def to_one_hot(num_categories, Y):\n",
        "        m = Y.shape[0]\n",
        "        Y = Y.reshape(1, m)\n",
        "        Y_new = np.eye(num_categories)[Y.astype('int32')]\n",
        "        Y_new = Y_new.T.reshape(num_categories, m)\n",
        "        return Y_new"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vig1dBxXoVDI"
      },
      "source": [
        "!cp /content/drive/MyDrive/Data/unit10/utils.py .\n",
        "import utils as u10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import h5py"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU4qRge0o7mP",
        "outputId": "b66a7c41-3658-4b06-9918-dc38e1ef9e07"
      },
      "source": [
        "np.random.seed(1)\n",
        "Y = np.random.rand(3, 5)\n",
        "Y = np.where(Y==Y.max(axis=0),1,0)\n",
        "Y_hat = np.random.rand(3, 5)\n",
        "Y_hat = Y_hat/np.sum(Y_hat,axis=0)\n",
        "print(f\"Y_hat:\\n{Y_hat}\\nY:\\n{Y}\\n\")\n",
        "model = DLModel(\"Check1\",None,\"categorical_cross_entropy\")\n",
        "print(f\"errors:\\n {model.loss_forward(Y_hat, Y)}\")\n",
        "print(f\"dZ:\\n {model.loss_backward(Y_hat, Y)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_hat:\n",
            "[[0.28339767 0.28376295 0.61315734 0.14003126 0.10145351]\n",
            " [0.33846404 0.65840786 0.34398037 0.69056859 0.44882427]\n",
            " [0.37813829 0.05782918 0.04286229 0.16940015 0.44972222]]\n",
            "Y:\n",
            "[[0 1 0 0 0]\n",
            " [0 0 1 0 1]\n",
            " [1 0 0 1 0]]\n",
            "\n",
            "errors:\n",
            " [[0.97249529 1.25961606 1.06717068 1.77549163 0.80112384]]\n",
            "dZ:\n",
            " [[ 0.05667953 -0.14324741  0.12263147  0.02800625  0.0202907 ]\n",
            " [ 0.06769281  0.13168157 -0.13120393  0.13811372 -0.11023515]\n",
            " [-0.12437234  0.01156584  0.00857246 -0.16611997  0.08994444]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-8aUJwmpBS_",
        "outputId": "25550cde-f8e3-41b5-bc84-04c7571b6a3d"
      },
      "source": [
        "np.random.seed(1)\n",
        "softmax_layer = DLActivation(\"softmax\")\n",
        "prev_A = np.random.randn(4, 5)\n",
        "A = softmax_layer.forward_propagation(prev_A)\n",
        "dA = A\n",
        "dA_prev = softmax_layer.backward_propagation(dA)\n",
        "print(\"A:\\n\",A)\n",
        "print(\"dA_prev:\\n\",dA_prev)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A:\n",
            " [[0.51664628 0.07495532 0.26842133 0.09935819 0.29501699]\n",
            " [0.01019069 0.79112846 0.21262344 0.39970995 0.09676206]\n",
            " [0.43927297 0.01761072 0.32974263 0.19787701 0.38582919]\n",
            " [0.03389006 0.1163055  0.18921259 0.30305485 0.22239176]]\n",
            "dA_prev:\n",
            " [[0.51664628 0.07495532 0.26842133 0.09935819 0.29501699]\n",
            " [0.01019069 0.79112846 0.21262344 0.39970995 0.09676206]\n",
            " [0.43927297 0.01761072 0.32974263 0.19787701 0.38582919]\n",
            " [0.03389006 0.1163055  0.18921259 0.30305485 0.22239176]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "DHp1QzqVpKBQ",
        "outputId": "2eb36f9c-34b6-49a5-8756-61f47adbc3fd"
      },
      "source": [
        "mnist = fetch_openml('mnist_784')\n",
        "X, Y = mnist[\"data\"], mnist[\"target\"] # np Arrays\n",
        "X = X / 255 - 0.5\n",
        "i = 12\n",
        "plt.imshow(X[i:i+1].reshape(28,28), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "print(\"Label is: '\"+Y[i]+\"'\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label is: '3'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcW1k4WVqIM1",
        "outputId": "e8300f71-a6d9-46a1-8d8a-72c9d10922a0"
      },
      "source": [
        "Y_new = DLModel.to_one_hot(10,Y)\n",
        "print(\"New label is:\", Y_new[:,i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New label is: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JQaBrG-WqPr1",
        "outputId": "4ec8b983-f6c3-4157-8358-c91874ae510d"
      },
      "source": [
        "m = 60000\n",
        "m_test = X.shape[0] - m\n",
        "X_train, X_test = X[:m].T, X[m:].T\n",
        "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
        "np.random.seed(111)\n",
        "shuffle_index = np.random.permutation(m)\n",
        "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n",
        "i = 12\n",
        "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "print(Y_train[:,i])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAFNklEQVR4nO3cMUubaxjH4eTYoR1ECkKh7VRcHF3FRQLiZyjFQsHJpSi66uxQpPQLdFLBQWzBrR0t2OLg4hfIokLsXsyZyzFPT30T80+8rjE3ud93+fGAD7HebrdrQJ5/+v0CwM3ECaHECaHECaHECaEe/GHuT7nQe/WbPnRyQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQihxQqgH/X6Bfvj+/XtxPjs7W5wvLi4W548fP/7rdxoEu7u7xXmz2SzO19fXi/O3b9/+9TsNMycnhBInhBInhBInhBInhBInhBInhKq32+3SvDhM9v79+46zlZWV4nd//frV7dehVquNjY0V51+/fu04m5qa6vbrJKnf9KGTE0KJE0KJE0KJE0KJE0KJE0KJE0IN7e85r6+vO87cY/bHz58/i/Otra2Os48fP3b7deI5OSGUOCGUOCGUOCGUOCGUOCGUOCHU0P6e88uXLx1ne3t7d/gmw+Pq6qo4397errR/fHy84+zi4qLS7nB+zwmDRJwQSpwQSpwQSpwQSpwQSpwQamjvOem+s7Oz4nxycrLS/s3NzY6z1dXVSrvDueeEQSJOCCVOCCVOCCVOCCVOCDW0/xqT7nv37l1P94+OjvZ0/6BxckIocUIocUIocUIocUIocUIocUIo95z8pvTvL09PTyvtfvjwYXH+4sWLSvuHjZMTQokTQokTQokTQokTQokTQokTQrnnvGdarVZx/vr1646zb9++VXr29PR0cT43N1dp/7BxckIocUIocUIocUIocUIocUIocUIo95z3zOHhYXH+6dOnW+9+9uxZcb67u3vr3feRkxNCiRNCiRNCiRNCiRNCiRNCiRNCueccMqX/O1ur1WpbW1s9e/bS0lJxPj4+3rNnDyMnJ4QSJ4QSJ4QSJ4QSJ4QSJ4RylTJklpeXi/Pj4+Nb756ZmSnO37x5c+vd/JeTE0KJE0KJE0KJE0KJE0KJE0KJE0K55xwwnz9/Ls739/cr7X/06FHH2cbGRvG7T548qfRsfufkhFDihFDihFDihFDihFDihFDihFDuOcP8+PGjOF9YWCjOW61WpeeX7jIbjUal3fwdJyeEEieEEieEEieEEieEEieEEieEcs8Z5vz8vDiveo85MTFRnL969arSfrrHyQmhxAmhxAmhxAmhxAmhxAmhXKWE2dvb6+n+xcXF4vzp06c9fT7/n5MTQokTQokTQokTQokTQokTQokTQrnn7IOTk5OOs4ODg0q719bWivOXL19W2s/dcXJCKHFCKHFCKHFCKHFCKHFCKHFCKPecffDhw4eOs8vLy0q75+fni/Pnz59X2s/dcXJCKHFCKHFCKHFCKHFCKHFCKHFCKPecfXB8fNyz3YeHh8X57Oxsz55Ndzk5IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZR7zj4YGRnp2e6jo6Oe7eZuOTkhlDghlDghlDghlDghlDghlKuUPtjZ2ek4azQaxe82m83i/E/fZ3A4OSGUOCGUOCGUOCGUOCGUOCGUOCFUvd1ul+bFIdAV9Zs+dHJCKHFCKHFCKHFCKHFCKHFCKHFCqD/9nvPG+xeg95ycEEqcEEqcEEqcEEqcEEqcEOpf262eVjZC8GUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5btHKPWyqWPO",
        "outputId": "38921858-110e-4acd-cbcc-33533d783d43"
      },
      "source": [
        "np.random.seed(1)\n",
        "hidden1 = DLNeuronsLayer(\"hidden1\",64,28*28, \"sigmoid\", alpha=0.1,optimization='adaptive',initialization='Xaviar')\n",
        "output = DLNeuronsLayer(\"output\",10,64, \"softmax\", alpha=0.1,optimization='adaptive',initialization = 'Xaviar')\n",
        "digit_network = DLNetwork(\"digit_network\")\n",
        "digit_network.add(hidden1)\n",
        "digit_network.add(output)\n",
        "print(digit_network)\n",
        "\n",
        "digit_model = DLModel(\"digit_model\",digit_network,\"categorical_cross_entropy\")\n",
        "print(digit_model)\n",
        "\n",
        "costs = digit_model.train(X_train, Y_train,200)\n",
        "u10.print_costs(costs,200)\n",
        "threshold = 0.7\n",
        "train_Y_hat = digit_model.forward_propagation(X_train) > threshold\n",
        "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(train_Y_hat - Y_train)) * 100))\n",
        "test_Y_hat = digit_model.forward_propagation(X_test) > threshold\n",
        "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(test_Y_hat - Y_test)) * 100))\n",
        "path = r'/content/drive/MyDrive/Data' ## change path to data location\n",
        "digit_network.save_parameters(path)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "digit_network:\n",
            "hidden1:\n",
            "linear Function:\n",
            "\tlearning_rate (alpha): 0.1\n",
            "\tOptimization: adaptive\n",
            "\t\tadaptive parameters:\n",
            "\t\t\tcont: 1.1\n",
            "\t\t\tswitch: 0.5\n",
            "\tParameters:\n",
            "\t\tW shape: (64, 784)\n",
            "\t\tb: (64, 1)\n",
            "\t\tnum units: 64\n",
            "\t\tinput size: 784\n",
            "Activation function: sigmoid\n",
            "output:\n",
            "linear Function:\n",
            "\tlearning_rate (alpha): 0.1\n",
            "\tOptimization: adaptive\n",
            "\t\tadaptive parameters:\n",
            "\t\t\tcont: 1.1\n",
            "\t\t\tswitch: 0.5\n",
            "\tParameters:\n",
            "\t\tW shape: (10, 64)\n",
            "\t\tb: (10, 1)\n",
            "\t\tnum units: 10\n",
            "\t\tinput size: 64\n",
            "Activation function: softmax\n",
            "\n",
            "digit_model\n",
            "\tLoss function: categorical_cross_entropy\n",
            "\tdigit_network:\n",
            "hidden1:\n",
            "linear Function:\n",
            "\tlearning_rate (alpha): 0.1\n",
            "\tOptimization: adaptive\n",
            "\t\tadaptive parameters:\n",
            "\t\t\tcont: 1.1\n",
            "\t\t\tswitch: 0.5\n",
            "\tParameters:\n",
            "\t\tW shape: (64, 784)\n",
            "\t\tb: (64, 1)\n",
            "\t\tnum units: 64\n",
            "\t\tinput size: 784\n",
            "Activation function: sigmoid\n",
            "output:\n",
            "linear Function:\n",
            "\tlearning_rate (alpha): 0.1\n",
            "\tOptimization: adaptive\n",
            "\t\tadaptive parameters:\n",
            "\t\t\tcont: 1.1\n",
            "\t\t\tswitch: 0.5\n",
            "\tParameters:\n",
            "\t\tW shape: (10, 64)\n",
            "\t\tb: (10, 1)\n",
            "\t\tnum units: 10\n",
            "\t\tinput size: 64\n",
            "Activation function: softmax\n",
            "\n",
            "\n",
            "cost after  3 updates (1%): 2.700538046470411\n",
            "cost after  5 updates (2%): 2.162242254168124\n",
            "cost after  7 updates (3%): 1.7754254666790243\n",
            "cost after  9 updates (4%): 1.3695668855335343\n",
            "cost after  11 updates (5%): 0.8654311886758567\n",
            "cost after  13 updates (6%): 0.6584961535162726\n",
            "cost after  15 updates (7%): 0.5376448795004556\n",
            "cost after  17 updates (8%): 0.4615639818025788\n",
            "cost after  19 updates (9%): 0.40317150282514935\n",
            "cost after  21 updates (10%): 0.3579202857712222\n",
            "cost after  23 updates (11%): 0.32281455377243456\n",
            "cost after  25 updates (12%): 0.2923674917506611\n",
            "cost after  27 updates (13%): 0.26754631923534283\n",
            "cost after  29 updates (14%): 0.2467936038920918\n",
            "cost after  31 updates (15%): 0.22831770262490814\n",
            "cost after  33 updates (16%): 0.21202414381316867\n",
            "cost after  35 updates (17%): 0.19770072952015402\n",
            "cost after  37 updates (18%): 0.18646905733227967\n",
            "cost after  39 updates (19%): 0.1764089403062226\n",
            "cost after  41 updates (20%): 0.1670425190280678\n",
            "cost after  43 updates (21%): 0.15887834453633012\n",
            "cost after  45 updates (22%): 0.15154590757966674\n",
            "cost after  47 updates (23%): 0.14469830146797089\n",
            "cost after  49 updates (24%): 0.13820768841436484\n",
            "cost after  51 updates (25%): 0.13200823688085578\n",
            "cost after  53 updates (26%): 0.12672291101732616\n",
            "cost after  55 updates (27%): 0.12249366486683891\n",
            "cost after  57 updates (28%): 0.11842914785589566\n",
            "cost after  59 updates (29%): 0.11446037683968162\n",
            "cost after  61 updates (30%): 0.11062727046337706\n",
            "cost after  63 updates (31%): 0.10695232373548849\n",
            "cost after  65 updates (32%): 0.1034835456079102\n",
            "cost after  67 updates (33%): 0.10025063828724091\n",
            "cost after  69 updates (34%): 0.0973926388691511\n",
            "cost after  71 updates (35%): 0.09463737735361594\n",
            "cost after  73 updates (36%): 0.09192260200720541\n",
            "cost after  75 updates (37%): 0.08927710897695233\n",
            "cost after  77 updates (38%): 0.08685512663409652\n",
            "cost after  79 updates (39%): 0.08470177400014214\n",
            "cost after  81 updates (40%): 0.08254624867411323\n",
            "cost after  83 updates (41%): 0.08045873611893606\n",
            "cost after  85 updates (42%): 0.07848913734060381\n",
            "cost after  87 updates (43%): 0.07664670329061067\n",
            "cost after  89 updates (44%): 0.07482007549620387\n",
            "cost after  91 updates (45%): 0.07302786917404866\n",
            "cost after  93 updates (46%): 0.07122599835011717\n",
            "cost after  95 updates (47%): 0.06947326796204538\n",
            "cost after  97 updates (48%): 0.06781605312092472\n",
            "cost after  99 updates (49%): 0.0663542729084517\n",
            "cost after  101 updates (50%): 0.06493519560097719\n",
            "cost after  103 updates (51%): 0.06348404416549755\n",
            "cost after  105 updates (52%): 0.06200852764323256\n",
            "cost after  107 updates (53%): 0.06059792859512145\n",
            "cost after  109 updates (54%): 0.05923269004322967\n",
            "cost after  111 updates (55%): 0.05800323980524573\n",
            "cost after  113 updates (56%): 0.056821728504571914\n",
            "cost after  115 updates (57%): 0.055630188778824495\n",
            "cost after  117 updates (58%): 0.05447177171470443\n",
            "cost after  119 updates (59%): 0.05335065542449496\n",
            "cost after  121 updates (60%): 0.0522583187895397\n",
            "cost after  123 updates (61%): 0.05118076372320833\n",
            "cost after  125 updates (62%): 0.050088211197979765\n",
            "cost after  127 updates (63%): 0.04898366570742732\n",
            "cost after  129 updates (64%): 0.04790231629579688\n",
            "cost after  131 updates (65%): 0.04689069941601862\n",
            "cost after  133 updates (66%): 0.045880198337971016\n",
            "cost after  135 updates (67%): 0.044905355689043025\n",
            "cost after  137 updates (68%): 0.044028859287939974\n",
            "cost after  139 updates (69%): 0.043159335260246665\n",
            "cost after  141 updates (70%): 0.04230986025011605\n",
            "cost after  143 updates (71%): 0.041447377209436545\n",
            "cost after  145 updates (72%): 0.04058566407459445\n",
            "cost after  147 updates (73%): 0.03976058478698277\n",
            "cost after  149 updates (74%): 0.03896070428098766\n",
            "cost after  151 updates (75%): 0.03817050262834326\n",
            "cost after  153 updates (76%): 0.03744228661795333\n",
            "cost after  155 updates (77%): 0.036784680379046274\n",
            "cost after  157 updates (78%): 0.03615215756429033\n",
            "cost after  159 updates (79%): 0.03551298980241216\n",
            "cost after  161 updates (80%): 0.03486337367297259\n",
            "cost after  163 updates (81%): 0.03421854108966081\n",
            "cost after  165 updates (82%): 0.03357840819475124\n",
            "cost after  167 updates (83%): 0.03294531846230223\n",
            "cost after  169 updates (84%): 0.0323597694218036\n",
            "cost after  171 updates (85%): 0.031766330556906155\n",
            "cost after  173 updates (86%): 0.031192936074694335\n",
            "cost after  175 updates (87%): 0.030614883672896675\n",
            "cost after  177 updates (88%): 0.03003516128802786\n",
            "cost after  179 updates (89%): 0.02944715389229998\n",
            "cost after  181 updates (90%): 0.02890809762549826\n",
            "cost after  183 updates (91%): 0.028404720489029093\n",
            "cost after  185 updates (92%): 0.027899213135920305\n",
            "cost after  187 updates (93%): 0.02737830430758111\n",
            "cost after  189 updates (94%): 0.026838924571168123\n",
            "cost after  191 updates (95%): 0.026282827080885085\n",
            "cost after  193 updates (96%): 0.025733522835088632\n",
            "cost after  195 updates (97%): 0.025216044609678925\n",
            "cost after  197 updates (98%): 0.024724548843428545\n",
            "cost after  199 updates (99%): 0.024261819756140703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "train accuracy: 99.86666666666666 %\n",
            "test accuracy: 99.213 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnaY4MEPvO-B",
        "outputId": "db2c0039-56ef-4dd2-8425-7b8377b5a781"
      },
      "source": [
        "path = r'/content/drive/MyDrive/Data' # change path to data location\n",
        "digit_network.restore_parameters(path)\n",
        "print(\"Train:\")\n",
        "digit_model.confusion_matrix(X_train, Y_train)\n",
        "print(\"Test:\")\n",
        "digit_model.confusion_matrix(X_test, Y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:\n",
            "accuracy:  0.9955833333333334\n",
            "[[5919    0    1    0    1    1    1    1    0    0]\n",
            " [   0 6730    2    2    4    1    0    4    0    1]\n",
            " [   0    5 5927   14    2    3    0   10    1    1]\n",
            " [   1    0    5 6078    1   18    0    2    3    4]\n",
            " [   1    0    3    0 5810    1    5    3    0    8]\n",
            " [   0    0    1   12    0 5382    0    0   12    6]\n",
            " [   0    0    0    0    3    3 5910    1    1    0]\n",
            " [   0    3   12   12    3    2    0 6237    1    9]\n",
            " [   1    4    5    7    2    5    2    2 5827    5]\n",
            " [   1    0    2    6   16    5    0    5    6 5915]]\n",
            "Test:\n",
            "accuracy:  0.9598\n",
            "[[ 960    0    6    1    0    2    5    1    6    1]\n",
            " [   0 1113    3    2    0    0    2    3    0    4]\n",
            " [   3    2  978   11    1    6    5    9    5    0]\n",
            " [   2    4   12  967    1   14    0   10    9    6]\n",
            " [   0    1    4    1  941    2    7    4    4   18]\n",
            " [   3    2    0   12    2  847    5    3    7    7]\n",
            " [   6    2    4    0    9    6  930    0    2    1]\n",
            " [   1    5   12    6    3    3    1  981    6   15]\n",
            " [   2    5   12    4    3    9    3    5  928    4]\n",
            " [   3    1    1    6   22    3    0   12    7  953]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pxFDir9vZUK",
        "outputId": "b3b7a8cf-84da-434a-e328-3797d9791fd6"
      },
      "source": [
        "#Test your image\n",
        "num_px = 28\n",
        "img_path = r'/content/drive/MyDrive/Data/eight.jpg' # 8 img\n",
        "image = Image.open(img_path)\n",
        "image28 = image.resize((num_px, num_px), Image.ANTIALIAS)\n",
        "gray_image = ImageOps.grayscale(image28)\n",
        "my_image = np.reshape(gray_image,(num_px*num_px,1))\n",
        "my_image = my_image/255 - 0.5\n",
        "prediction = digit_model.forward_propagation(my_image)\n",
        "print(f\"prediction_index = {np.argmax(prediction, axis=0)}\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction_index = [7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg-SaUa50xsp",
        "outputId": "fc694a45-8b73-4d6d-bbd6-cd52fa036b35"
      },
      "source": [
        "#Test your image\n",
        "num_px = 28\n",
        "img_path = r'/content/drive/MyDrive/Data/two.jpg' # 2 img\n",
        "image = Image.open(img_path)\n",
        "image28 = image.resize((num_px, num_px), Image.ANTIALIAS)\n",
        "gray_image = ImageOps.grayscale(image28)\n",
        "my_image = np.reshape(gray_image,(num_px*num_px,1))\n",
        "my_image = my_image/255 - 0.5\n",
        "prediction = digit_model.forward_propagation(my_image)\n",
        "print(f\"prediction_index = {np.argmax(prediction, axis=0)}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction_index = [7]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}